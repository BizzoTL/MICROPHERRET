{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc674bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 12:45:11.150015: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 12:45:11.412135: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 12:45:11.464632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:11.464642: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-05 12:45:12.236560: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:12.236694: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:12.236697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baccus/.local/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "2023-12-05 12:45:13.304501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-05 12:45:13.304640: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:13.304678: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:13.304701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:13.304722: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:13.304742: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:13.304761: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:13.304780: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:13.304799: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-12-05 12:45:13.304802: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-12-05 12:45:13.306221: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import argparse\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,  StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from pickle import dump\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.metrics import CategoricalAccuracy, AUC, BinaryCrossentropy, MeanSquaredError, TruePositives, FalsePositives, TrueNegatives,FalseNegatives, BinaryAccuracy,Precision,Recall,AUC\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import keras_tuner\n",
    "from math import sqrt\n",
    "from pickle import dump, load\n",
    "import keras.backend as K\n",
    "\n",
    "def matthews_correlation_coefficient(y_true, y_pred):\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / K.sqrt(den + K.epsilon())\n",
    "\n",
    "METRICS = [BinaryCrossentropy(name='Binary crossentropy'),  \n",
    "      MeanSquaredError(name='Brier score'),\n",
    "      TruePositives(name='tp'),\n",
    "      FalsePositives(name='fp'),\n",
    "      TrueNegatives(name='tn'),\n",
    "      FalseNegatives(name='fn'), \n",
    "      BinaryAccuracy(name='accuracy'),\n",
    "      Precision(name='precision'),\n",
    "      Recall(name='recall'),\n",
    "      AUC(name='auc'),\n",
    "      AUC(name='prc', curve='PR'),\n",
    "        matthews_correlation_coefficient\n",
    "    # precision-recall curve\n",
    "]\n",
    "\n",
    "classes = [\"anoxygenic_photoautotrophy_Fe_oxidizing\",\n",
    "\"dark_sulfite_oxidation\",\n",
    "\"knallgas_bacteria\",\n",
    "\"oil_bioremediation\",\n",
    "\"dark_sulfur_oxidation\",\n",
    "\"dark_thiosulfate_oxidation\",\n",
    "\"anoxygenic_photoautotrophy_S_oxidizing\"\n",
    "]\n",
    "dataset = pd.read_csv('../Matrices/ko_df_validation.csv').set_index('Unnamed: 0')\n",
    "validation_ds = pd.read_csv('../Matrices/ko_df_frag50.csv').set_index('Unnamed: 0')\n",
    "validation_ds = validation_ds.sort_index()\n",
    "functions = pd.read_csv(\"../Matrices/function_db.csv\").set_index('Unnamed: 0')\n",
    "def get_validation_set(to_validate, training_set):\n",
    "    training_kos = training_set.columns\n",
    "    validation_kos = to_validate.columns\n",
    "    if len(set(training_kos).intersection(set(validation_kos))) == 0:\n",
    "        print('no common kos between provided ones and training')\n",
    "        return\n",
    "    else:\n",
    "        common = list(set(training_kos).intersection(set(validation_kos)))\n",
    "        print('{} common kos'.format(len(common)))\n",
    "        common_table = to_validate[common]\n",
    "        #remove orthologs in validation not in the training\n",
    "        to_remove = set(validation_kos) - set(training_kos)\n",
    "        print('{} kos present in user set but not in training set will be removed'.format(len(to_remove)))\n",
    "        missing = list(set(training_kos) - set(validation_kos))\n",
    "        print('{} kos missing from the users et will be add to train the classifiers'.format(len(missing)))\n",
    "        #missing_df = pd\n",
    "        missed = pd.DataFrame(0, index = to_validate.index, columns= missing)\n",
    "        to_submit = common_table.merge(missed, left_index = True, right_index = True)\n",
    "        #print(list(validation_kos))\n",
    "        to_submit = to_submit[list(training_kos)] #change order columns\n",
    "        print('Shape of training dataset: {}, Shape of user dataset: {}'.format(training_set.shape, to_submit.shape))\n",
    "        if list(to_submit.columns) != list(training_set.columns): \n",
    "            print('ERRORRRRR')\n",
    "            return\n",
    "    return to_submit\n",
    "\n",
    "validation = get_validation_set(validation_ds, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../Matrices/genome_ko_all.csv').set_index('Genome')\n",
    "validation_ds = pd.read_csv('../Matrices/ko_df_frag50.csv').set_index('Unnamed: 0')\n",
    "validation_ds = validation_ds.drop([\"GCF_000187005.1\"])\n",
    "validation_ds = validation_ds.sort_index()\n",
    "functions = pd.read_csv(\"../Matrices/function_db.csv\").set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24ffcca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9564 common kos\n",
      "206 kos present in user set but not in training set will be removed\n",
      "1906 kos missing from the users et will be add to train the classifiers\n",
      "Shape of training dataset: (14363, 11470), Shape of user dataset: (4146, 11470)\n",
      "130/130 [==============================] - 0s 2ms/step\n",
      "9564 common kos\n",
      "206 kos present in user set but not in training set will be removed\n",
      "1906 kos missing from the users et will be add to train the classifiers\n",
      "Shape of training dataset: (14363, 11470), Shape of user dataset: (4146, 11470)\n",
      "130/130 [==============================] - 0s 2ms/step\n",
      "9564 common kos\n",
      "206 kos present in user set but not in training set will be removed\n",
      "1906 kos missing from the users et will be add to train the classifiers\n",
      "Shape of training dataset: (14363, 11470), Shape of user dataset: (4146, 11470)\n",
      "130/130 [==============================] - 0s 2ms/step\n",
      "9564 common kos\n",
      "206 kos present in user set but not in training set will be removed\n",
      "1906 kos missing from the users et will be add to train the classifiers\n",
      "Shape of training dataset: (14363, 11470), Shape of user dataset: (4146, 11470)\n",
      "130/130 [==============================] - 0s 2ms/step\n",
      "9564 common kos\n",
      "206 kos present in user set but not in training set will be removed\n",
      "1906 kos missing from the users et will be add to train the classifiers\n",
      "Shape of training dataset: (14363, 11470), Shape of user dataset: (4146, 11470)\n",
      "130/130 [==============================] - 0s 2ms/step\n",
      "9564 common kos\n",
      "206 kos present in user set but not in training set will be removed\n",
      "1906 kos missing from the users et will be add to train the classifiers\n",
      "Shape of training dataset: (14363, 11470), Shape of user dataset: (4146, 11470)\n",
      "130/130 [==============================] - 0s 2ms/step\n",
      "9564 common kos\n",
      "206 kos present in user set but not in training set will be removed\n",
      "1906 kos missing from the users et will be add to train the classifiers\n",
      "Shape of training dataset: (14363, 11470), Shape of user dataset: (4146, 11470)\n",
      "130/130 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "functions_2 = pd.read_csv(\"../Matrices/predict_functions.csv\").set_index('Unnamed: 0')\n",
    "for classe in classes:\n",
    "    model_load = \"./validations/final_\"+classe+\".mdl_wts.hdf5\"\n",
    "    scaler = load(open('./validations/scaler_'+classe+'.sav', 'rb'))\n",
    "    modelo = load_model(model_load, compile = True, custom_objects={\"matthews_correlation_coefficient\":matthews_correlation_coefficient})\n",
    "    validation = get_validation_set(validation_ds, dataset)\n",
    "    training_dataset_1 = validation.drop(columns=[validation.columns[0]])\n",
    "    training_dataset_x = scaler.transform(training_dataset_1.values.reshape(training_dataset_1.shape[0],training_dataset_1.shape[1]))\n",
    "    his = (modelo.predict(training_dataset_x) > 0.5).astype(np.int32)\n",
    "    his2 = []\n",
    "    results_TN = 0\n",
    "    results_TP = 0\n",
    "    results_FP = 0\n",
    "    results_FN = 0\n",
    "    for i in range(len(his)):\n",
    "        \n",
    "        if his[i] == 0:\n",
    "            if functions_2[classe].values[i] == 0:\n",
    "                results_TN += 1\n",
    "            else:\n",
    "                results_FN += 1\n",
    "        else:\n",
    "            if functions_2[classe].values[i] == 0:\n",
    "                results_FP += 1\n",
    "            else:\n",
    "                results_TP += 1\n",
    "        his2.append([his[i],functions_2[classe].values[i]])\n",
    "        \n",
    "\n",
    "    his2.append([results_TP, results_TN, results_FP, results_FN])\n",
    "    his2 = pd.DataFrame(his2)\n",
    "    names = \"predict_\"+classe+\".csv\"\n",
    "    his2.to_csv(names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
